{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ebe388f",
   "metadata": {},
   "source": [
    "# ğŸ“Š Data Preparation\n",
    "\n",
    "This notebook handles the full data preparation process:\n",
    "- Reading service call data, events (holidays), and weather information\n",
    "- Merging datasets by date\n",
    "- Creating new featur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d98c724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if needed\n",
    "# !pip install pandas matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad3c147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Calls shape: (97159, 14)\n",
      "Events shape: (821, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                ××’×£            ××—×œ×§×”                    × ×•×©×  \\\n",
       " 0         ××’×£ ×ª×¤×¢×•×œ     ××—×œ×§×ª ×ª×‘×¨×•××”              ××¡×¤×§×ª ×¤×—×™×   \n",
       " 1         ××’×£ ×ª×¤×¢×•×œ     ××—×œ×§×ª ×ª×‘×¨×•××”  ×“×™×•×•×— ×¢×œ ××™ ×¤×™× ×•×™ ×¤×—×™×   \n",
       " 2         ××’×£ ×ª×¤×¢×•×œ       ××“×•×¨ ×§× ×¡×•×ª      ×¡×•×’×™×•×ª ×—× ×™×” ×•×“×•×—×•×ª   \n",
       " 3  ×œ×©×›×ª ×¨××©×ª ×”××•×¢×¦×”  ×œ×©×›×ª ×¨××©×ª ××•×¢×¦×”       ×ª×œ×•× ×•×ª/×‘×§×©×•×ª ×›×œ×œ×™   \n",
       " 4         ××’×£ ×ª×¤×¢×•×œ     ××—×œ×§×ª ×ª×‘×¨×•××”  ×“×™×•×•×— ×¢×œ ××™ ×¤×™× ×•×™ ×¤×—×™×   \n",
       " \n",
       "                                            × ×•×©× ××©× ×”    ×¡×˜×˜×•×¡ ×¤× ×™×™×”  \\\n",
       " 0                          ×ª×‘×¨×•××” - ×¤×— ×™×¨×•×§ ×—×“×©/×©×‘×•×¨  ×”×˜×™×¤×•×œ ×”×¡×ª×™×™×   \n",
       " 1                          ×ª×‘×¨×•××” - ××™ ×¤×™× ×•×™ ×¤×— ×™×¨×•×§  ×”×˜×™×¤×•×œ ×”×¡×ª×™×™×   \n",
       " 2  ×—× ×™×” - ×‘×™×¨×•×¨×™× ×¢×œ ×—×•×‘ / ×¢×™×§×•×œ/×“×—×™×™×ª ×¢×¨×¢×•×¨/×‘×™×¨×•...  ×”×˜×™×¤×•×œ ×”×¡×ª×™×™×   \n",
       " 3                                       ×¤× ×™×•×ª ×ª×•×©×‘×™×  ×”×˜×™×¤×•×œ ×”×¡×ª×™×™×   \n",
       " 4                          ×ª×‘×¨×•××” - ××™ ×¤×™× ×•×™ ×¤×— ×›×ª×•×  ×”×˜×™×¤×•×œ ×”×¡×ª×™×™×   \n",
       " \n",
       "   ×—×¨×™×’×” ×‘×©×¢×•×ª ×¢×‘×•×“×” (×¤× ×™×•×ª ×¡×’×•×¨×•×ª ×‘×œ×‘×“) ××—×•×– ×—×¨×™×’×” (×¤× ×™×•×ª ×¡×’×•×¨×•×ª ×‘×œ×‘×“)  \\\n",
       " 0                           55.32885683                    0.232215083   \n",
       " 1                                   0.0                            0.0   \n",
       " 2                                   0.0                            0.0   \n",
       " 3                           0.358679508                            0.0   \n",
       " 4                           84.02519033                    1.848342497   \n",
       " \n",
       "    ××©×š ×˜×™×¤×•×œ (×‘×©×¢×•×ª ×¢×‘×•×“×”) ×ª××¨×™×š ×¤×ª×™×—×” ×©×¢×ª ×¤×ª×™×—×”       ×ª××¨×™×š ×¡×’×™×¨×”  \\\n",
       " 0                65.017618  01/01/2019     01:19  01/01/2019 01:19   \n",
       " 1                26.360072  01/01/2019     04:01  01/01/2019 04:01   \n",
       " 2                 0.000000  01/01/2019     08:35  01/01/2019 08:35   \n",
       " 3                 0.000000  01/01/2019     09:00  01/01/2019 09:00   \n",
       " 4               128.956738  01/01/2019     16:33  01/01/2019 16:33   \n",
       " \n",
       "    ×ª××¨×™×š ×™×¢×“ ×œ×¡×’×™×¨×”       ×™×™×©×•×‘         ×¨×—×•×‘  \n",
       " 0  18/01/2019 18:29  ×’××•×œ×™ ×ª×™××Ÿ          NaN  \n",
       " 1  01/01/2019 04:01    ×’×Ÿ ×™××©×™×”  ×”×›×‘×™×© ×”×¨××©×™  \n",
       " 2  28/02/2019 22:19    ×‘×™×ª ×”×œ×•×™     ×‘×™×ª ×”×œ×•×™  \n",
       " 3  01/01/2019 09:00      ××›××•×¨×ª      ×“×¨×š ×”×™×  \n",
       " 4  26/02/2019 09:15      ×”××¢×¤×™×œ          NaN  ,\n",
       "         ×ª××¨×™×š     ××™×¨×•×¢     ×¡×™×•×•×’\n",
       " 0  2019-01-05  ×™×•× ×©×™×©×™  ×™×•× ×¨×’×™×œ\n",
       " 1  2019-01-06       ×©×‘×ª        ×—×’\n",
       " 2  2019-01-12  ×™×•× ×©×™×©×™  ×™×•× ×¨×’×™×œ\n",
       " 3  2019-01-13       ×©×‘×ª        ×—×’\n",
       " 4  2019-01-19  ×™×•× ×©×™×©×™  ×™×•× ×¨×’×™×œ)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Service Calls and Events\n",
    "data = pd.read_csv('../data/data_filled.csv', low_memory=False)\n",
    "events = pd.read_csv('../data/events_final.csv', low_memory=False)\n",
    "\n",
    "print(f\"Service Calls shape: {data.shape}\")\n",
    "print(f\"Events shape: {events.shape}\")\n",
    "\n",
    "# Display first rows\n",
    "data.head(), events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f105ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "data['×ª××¨×™×š ×¤×ª×™×—×”'] = pd.to_datetime(data['×ª××¨×™×š ×¤×ª×™×—×”'], dayfirst=True, errors='coerce')\n",
    "events['×ª××¨×™×š'] = pd.to_datetime(events['×ª××¨×™×š'], dayfirst=True, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b849b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged Service Calls + Events\n"
     ]
    }
   ],
   "source": [
    "# Merge data with events\n",
    "merged = pd.merge(\n",
    "    data,\n",
    "    events,\n",
    "    left_on='×ª××¨×™×š ×¤×ª×™×—×”',\n",
    "    right_on='×ª××¨×™×š',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop duplicate '×ª××¨×™×š' column\n",
    "merged.drop(columns=['×ª××¨×™×š'], inplace=True)\n",
    "\n",
    "# Save intermediate merged file\n",
    "merged.to_csv('../data/data_merged.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… Merged Service Calls + Events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "182046c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged with Weather Data\n"
     ]
    }
   ],
   "source": [
    "# Merge merged service calls with weather\n",
    "final = pd.merge(\n",
    "    merged,\n",
    "    weather,\n",
    "    on='×ª××¨×™×š ×¤×ª×™×—×”',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the final merged dataset\n",
    "final.to_csv('../data/data_final_with_weather.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… Merged with Weather Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e998c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split '×ª××¨×™×š ×¡×’×™×¨×”' into '×ª××¨×™×š ×¡×’×™×¨×”' (date) and '×©×¢×ª ×¡×’×™×¨×”' (time)\n"
     ]
    }
   ],
   "source": [
    "# Load the final merged data\n",
    "data = pd.read_csv('../data/data_final_with_weather.csv', low_memory=False)\n",
    "\n",
    "# Convert '×ª××¨×™×š ×¡×’×™×¨×”' to datetime\n",
    "data['×ª××¨×™×š ×¡×’×™×¨×”'] = pd.to_datetime(data['×ª××¨×™×š ×¡×’×™×¨×”'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Create separate date and time columns\n",
    "data['×©×¢×ª ×¡×’×™×¨×”'] = data['×ª××¨×™×š ×¡×’×™×¨×”'].dt.strftime('%H:%M')\n",
    "data['×ª××¨×™×š ×¡×’×™×¨×”'] = data['×ª××¨×™×š ×¡×’×™×¨×”'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Save after splitting\n",
    "data.to_csv('../data/data_final_with_weather1.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… Split '×ª××¨×™×š ×¡×’×™×¨×”' into '×ª××¨×™×š ×¡×’×™×¨×”' (date) and '×©×¢×ª ×¡×’×™×¨×”' (time)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c36e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calculated Treatment Duration\n"
     ]
    }
   ],
   "source": [
    "# Load updated data\n",
    "data = pd.read_csv('../data/data_final_with_weather1.csv', low_memory=False)\n",
    "\n",
    "# Fix column names if needed\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Convert dates and times\n",
    "data['×ª××¨×™×š ×¤×ª×™×—×”'] = pd.to_datetime(data['×ª××¨×™×š ×¤×ª×™×—×”'], dayfirst=True, errors='coerce')\n",
    "data['×ª××¨×™×š ×¡×’×™×¨×”'] = pd.to_datetime(data['×ª××¨×™×š ×¡×’×™×¨×”'], dayfirst=True, errors='coerce')\n",
    "\n",
    "data['×©×¢×ª ×¤×ª×™×—×”'] = pd.to_datetime(data['×©×¢×ª ×¤×ª×™×—×”'], format='%H:%M', errors='coerce').dt.time\n",
    "data['×©×¢×ª ×¡×’×™×¨×”'] = pd.to_datetime(data['×©×¢×ª ×¡×’×™×¨×”'], format='%H:%M', errors='coerce').dt.time\n",
    "\n",
    "# Function to safely combine date and time\n",
    "def safe_combine(date, time):\n",
    "    if pd.notna(date) and pd.notna(time):\n",
    "        return pd.Timestamp.combine(date, time)\n",
    "    else:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply combination\n",
    "data['×¤×ª×™×—×” - datetime'] = data.apply(lambda row: safe_combine(row['×ª××¨×™×š ×¤×ª×™×—×”'], row['×©×¢×ª ×¤×ª×™×—×”']), axis=1)\n",
    "data['×¡×’×™×¨×” - datetime'] = data.apply(lambda row: safe_combine(row['×ª××¨×™×š ×¡×’×™×¨×”'], row['×©×¢×ª ×¡×’×™×¨×”']), axis=1)\n",
    "\n",
    "# Calculate Treatment Duration\n",
    "data['××©×š ×˜×™×¤×•×œ (×‘×©×¢×•×ª)'] = (data['×¡×’×™×¨×” - datetime'] - data['×¤×ª×™×—×” - datetime']).dt.total_seconds() / 3600\n",
    "\n",
    "# Save\n",
    "data.to_csv('../data/data_with_duration.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… Calculated Treatment Duration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a354e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added Hebrew Weekday Column - Final dataset ready!\n"
     ]
    }
   ],
   "source": [
    "# Add weekday column (in Hebrew)\n",
    "data['×ª××¨×™×š ×¤×ª×™×—×”'] = pd.to_datetime(data['×ª××¨×™×š ×¤×ª×™×—×”'], errors='coerce')\n",
    "\n",
    "# Get weekday name\n",
    "data['×™×•× ×‘×©×‘×•×¢'] = data['×ª××¨×™×š ×¤×ª×™×—×”'].dt.day_name()\n",
    "\n",
    "# Translate to Hebrew\n",
    "translation = {\n",
    "    'Sunday': '×¨××©×•×Ÿ',\n",
    "    'Monday': '×©× ×™',\n",
    "    'Tuesday': '×©×œ×™×©×™',\n",
    "    'Wednesday': '×¨×‘×™×¢×™',\n",
    "    'Thursday': '×—××™×©×™',\n",
    "    'Friday': '×©×™×©×™',\n",
    "    'Saturday': '×©×‘×ª'\n",
    "}\n",
    "data['×™×•× ×‘×©×‘×•×¢'] = data['×™×•× ×‘×©×‘×•×¢'].map(translation)\n",
    "\n",
    "# Save final file\n",
    "data.to_csv('../data/file_with_weekday.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… Added Hebrew Weekday Column - Final dataset ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716b7a7",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The dataset has been successfully:\n",
    "- Cleaned\n",
    "- Enhanced with new features (day of the week, treatment duration)\n",
    "- Scaled for model training\n",
    "\n",
    "We also explored the distributions of key features, providing initial insights into the data behavior.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
